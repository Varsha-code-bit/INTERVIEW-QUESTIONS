{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Varsha-code-bit/INTERVIEW-QUESTIONS/blob/main/Copy_of_Welcome_To_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install wolta\n",
        "!pip install ultralytics\n",
        "!pip install transformers\n",
        "!pip install torch torchvision\n",
        "!pip install tensorflow\n",
        "!pip install hyperopt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iM4DrXdmEOhz",
        "outputId": "bf036178-0214-41e6-a4f4-83caa037562e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: wolta in /usr/local/lib/python3.12/dist-packages (0.3.7)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (from wolta) (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from wolta) (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from wolta) (2.0.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.12/dist-packages (from wolta) (0.2.7)\n",
            "Requirement already satisfied: catboost in /usr/local/lib/python3.12/dist-packages (from wolta) (1.2.8)\n",
            "Requirement already satisfied: imblearn in /usr/local/lib/python3.12/dist-packages (from wolta) (0.0)\n",
            "Requirement already satisfied: lightgbm in /usr/local/lib/python3.12/dist-packages (from wolta) (4.6.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from wolta) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from wolta) (4.12.0.88)\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.12/dist-packages (from catboost->wolta) (0.21)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from catboost->wolta) (1.16.1)\n",
            "Requirement already satisfied: plotly in /usr/local/lib/python3.12/dist-packages (from catboost->wolta) (5.24.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from catboost->wolta) (1.17.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas->wolta) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->wolta) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->wolta) (2025.2)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.12/dist-packages (from hyperopt->wolta) (3.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from hyperopt->wolta) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from hyperopt->wolta) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from hyperopt->wolta) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (from hyperopt->wolta) (0.10.9.7)\n",
            "Requirement already satisfied: imbalanced-learn in /usr/local/lib/python3.12/dist-packages (from imblearn->wolta) (0.14.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->wolta) (3.2.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->wolta) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn->wolta) (3.6.0)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.12/dist-packages (from plotly->catboost->wolta) (8.5.0)\n",
            "Requirement already satisfied: ultralytics in /usr/local/lib/python3.12/dist-packages (8.3.202)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (4.12.0.88)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (11.3.0)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.32.4)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.16.1)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (0.23.0+cu126)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: polars in /usr/local/lib/python3.12/dist-packages (from ultralytics) (1.25.2)\n",
            "Requirement already satisfied: ultralytics-thop>=2.0.0 in /usr/local/lib/python3.12/dist-packages (from ultralytics) (2.0.17)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.59.2)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (25.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests>=2.23.0->ultralytics) (2025.8.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch>=1.8.0->ultralytics) (3.4.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.56.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.19.1)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.34.4)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.0)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.12/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.3.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.15.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (1.1.9)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.8.3)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch) (3.19.1)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.74.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: hyperopt in /usr/local/lib/python3.12/dist-packages (0.2.7)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (from hyperopt) (2.0.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.16.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.17.0)\n",
            "Requirement already satisfied: networkx>=2.2 in /usr/local/lib/python3.12/dist-packages (from hyperopt) (3.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.12/dist-packages (from hyperopt) (1.0.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from hyperopt) (4.67.1)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from hyperopt) (3.1.1)\n",
            "Requirement already satisfied: py4j in /usr/local/lib/python3.12/dist-packages (from hyperopt) (0.10.9.7)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import cv2\n",
        "import os\n",
        "from glob import glob\n",
        "from pathlib import Path\n",
        "import zipfile\n",
        "\n",
        "from wolta.visual_tools import get_extensions, dataset_size_same, dataset_ratio_same, examine_sizes, dir_split\n",
        "from ultralytics import YOLO\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "from transformers import ViTForImageClassification\n",
        "from torch import nn\n",
        "from torch.optim import Adam\n",
        "from tensorflow.keras import layers, Sequential\n",
        "\n",
        "os.environ['WANDB_MODE'] = 'disabled'\n"
      ],
      "metadata": {
        "id": "IcIFBANPEQ05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Paths\n",
        "zip_path = '/content/dataset_ISL_.zip'\n",
        "extract_path = '/content/isl_dataset'\n",
        "\n",
        "# Extract ZIP\n",
        "with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "    zip_ref.extractall(extract_path)\n",
        "print(\"Files extracted to:\", extract_path)\n",
        "\n",
        "# Check structure\n",
        "for dirname, _, _ in os.walk(extract_path):\n",
        "    print(dirname)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NXwcLXD1EciY",
        "outputId": "27c3aed7-39c8-4cf0-9201-f5c0c296cf59"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Files extracted to: /content/isl_dataset\n",
            "/content/isl_dataset\n",
            "/content/isl_dataset/o\n",
            "/content/isl_dataset/d\n",
            "/content/isl_dataset/v\n",
            "/content/isl_dataset/s\n",
            "/content/isl_dataset/a\n",
            "/content/isl_dataset/m\n",
            "/content/isl_dataset/y\n",
            "/content/isl_dataset/k\n",
            "/content/isl_dataset/u\n",
            "/content/isl_dataset/h\n",
            "/content/isl_dataset/e\n",
            "/content/isl_dataset/f\n",
            "/content/isl_dataset/n\n",
            "/content/isl_dataset/t\n",
            "/content/isl_dataset/q\n",
            "/content/isl_dataset/l\n",
            "/content/isl_dataset/j\n",
            "/content/isl_dataset/x\n",
            "/content/isl_dataset/r\n",
            "/content/isl_dataset/p\n",
            "/content/isl_dataset/w\n",
            "/content/isl_dataset/i\n",
            "/content/isl_dataset/b\n",
            "/content/isl_dataset/g\n",
            "/content/isl_dataset/c\n",
            "/content/isl_dataset/z\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from glob import glob\n",
        "import os\n",
        "\n",
        "extract_path = '/content/isl_dataset'  # Ensure extract_path is defined\n",
        "\n",
        "# Use glob to find all files within the extracted directory and its subdirectories\n",
        "all_paths = glob(os.path.join(extract_path, '**/*'), recursive=True)\n",
        "\n",
        "# Filter the list to include only files\n",
        "i_paths = [path for path in all_paths if os.path.isfile(path)]\n",
        "\n",
        "print(\"Total images:\", len(i_paths))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "msCm9jqhEjH9",
        "outputId": "ffc4b89e-de0e-40bc-d569-71e2d32311c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total images: 12637\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from wolta.visual_tools import get_extensions, dataset_size_same, dataset_ratio_same, examine_sizes\n",
        "\n",
        "get_extensions(i_paths)\n",
        "dataset_size_same(i_paths)\n",
        "dataset_ratio_same(i_paths)\n",
        "examine_sizes(i_paths)\n",
        "\n",
        "temp_img = cv2.imread(i_paths[0])\n",
        "height, width = temp_img.shape[:2]\n",
        "print('ratio: {}'.format((width/height)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eumLBe2GFOnB",
        "outputId": "55732c22-4549-4f2a-cbf1-6a557855a134"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average Width x Height: 102.64532721373743 x 103.48642874099866\n",
            "Biggest width is 1836 and the smallest width is 100\n",
            "Biggest height is 2314 and the smallest height is 100\n",
            "ratio: 1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "raw_dir = '/content/isl_dataset/raw'\n",
        "os.makedirs(raw_dir, exist_ok=True)\n",
        "\n",
        "# Define d_paths by listing directories within extract_path, excluding 'raw' and 'data'\n",
        "extract_path = '/content/isl_dataset'\n",
        "d_paths = [os.path.join(extract_path, d) for d in os.listdir(extract_path) if os.path.isdir(os.path.join(extract_path, d)) and d not in ['raw', 'data']]\n",
        "\n",
        "\n",
        "for d_path in d_paths:\n",
        "    class_name = Path(d_path).name\n",
        "    class_dir = os.path.join(raw_dir, class_name)\n",
        "    os.makedirs(class_dir, exist_ok=True)\n",
        "\n",
        "    # Use glob to find only files within the directory\n",
        "    img_paths = glob(os.path.join(d_path, '*.*')) # Added .* to only match files with extensions\n",
        "    for img_path in img_paths:\n",
        "        img_name = Path(img_path).name\n",
        "        img = cv2.imread(img_path)\n",
        "        # Add a check to ensure img is not None before resizing\n",
        "        if img is not None:\n",
        "            img = cv2.resize(img, (224, 224))\n",
        "            cv2.imwrite(os.path.join(class_dir, img_name), img)\n",
        "        else:\n",
        "            print(f\"Warning: Could not read image file: {img_path}\")"
      ],
      "metadata": {
        "id": "ENzYEuChI70G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import shutil\n",
        "\n",
        "data_dir = '/content/isl_dataset/data'\n",
        "\n",
        "# Remove the data directory if it exists\n",
        "if os.path.exists(data_dir):\n",
        "    shutil.rmtree(data_dir)\n",
        "\n",
        "dir_split(raw_dir, data_dir, val_size=0.2, test_size=0.2)\n",
        "\n",
        "train_dir = os.path.join(data_dir, 'train')\n",
        "val_dir = os.path.join(data_dir, 'val')\n",
        "test_dir = os.path.join(data_dir, 'test')\n",
        "\n",
        "print(\"Train classes:\", os.listdir(train_dir))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGOk8FyXJA_8",
        "outputId": "562c2759-d186-4e54-ddbd-07c25f83a45e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train classes: ['o', 'd', 'v', 's', 'a', 'm', 'y', 'k', 'u', 'h', 'e', 'f', 'n', 't', 'q', 'l', 'j', 'x', 'r', 'p', 'w', 'i', 'b', 'g', 'c', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "yolo_model = YOLO(model='yolo11x-cls.pt')\n",
        "yolo_results = yolo_model.train(data=data_dir, epochs=5, imgsz=224)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mpSjgyg5JpMR",
        "outputId": "12780fad-2644-479f-fc71-421097436ca8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11x-cls.pt to 'yolo11x-cls.pt': 100% ━━━━━━━━━━━━ 56.9MB 92.5MB/s 0.6s\n",
            "Ultralytics 8.3.202 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, compile=False, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=/content/isl_dataset/data, degrees=0.0, deterministic=True, device=None, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=5, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=224, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolo11x-cls.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=train, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=100, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/content/runs/classify/train, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=classify, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/isl_dataset/data/train... found 7593 images in 26 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/isl_dataset/data/val... found 2522 images in 26 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/isl_dataset/data/test... found 2522 images in 26 classes ✅ \n",
            "Overriding model.yaml nc=80 with nc=26\n",
            "\n",
            "                   from  n    params  module                                       arguments                     \n",
            "  0                  -1  1      2784  ultralytics.nn.modules.conv.Conv             [3, 96, 3, 2]                 \n",
            "  1                  -1  1    166272  ultralytics.nn.modules.conv.Conv             [96, 192, 3, 2]               \n",
            "  2                  -1  2    389760  ultralytics.nn.modules.block.C3k2            [192, 384, 2, True, 0.25]     \n",
            "  3                  -1  1   1327872  ultralytics.nn.modules.conv.Conv             [384, 384, 3, 2]              \n",
            "  4                  -1  2   1553664  ultralytics.nn.modules.block.C3k2            [384, 768, 2, True, 0.25]     \n",
            "  5                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  6                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  7                  -1  1   5309952  ultralytics.nn.modules.conv.Conv             [768, 768, 3, 2]              \n",
            "  8                  -1  2   5022720  ultralytics.nn.modules.block.C3k2            [768, 768, 2, True]           \n",
            "  9                  -1  2   3264768  ultralytics.nn.modules.block.C2PSA           [768, 768, 2]                 \n",
            " 10                  -1  1   1018906  ultralytics.nn.modules.head.Classify         [768, 26]                     \n",
            "YOLO11x-cls summary: 176 layers, 28,389,370 parameters, 28,389,370 gradients, 111.0 GFLOPs\n",
            "Transferred 492/494 items from pretrained weights\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mrunning Automatic Mixed Precision (AMP) checks...\n",
            "\u001b[KDownloading https://github.com/ultralytics/assets/releases/download/v8.3.0/yolo11n.pt to 'yolo11n.pt': 100% ━━━━━━━━━━━━ 5.4MB 83.3MB/s 0.1s\n",
            "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ✅\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 484.1±170.5 MB/s, size: 14.2 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mtrain: \u001b[0mScanning /content/isl_dataset/data/train... 7593 images, 0 corrupt: 100% ━━━━━━━━━━━━ 7593/7593 3.1Kit/s 2.4s\n",
            "\u001b[34m\u001b[1mtrain: \u001b[0mNew cache created: /content/isl_dataset/data/train.cache\n",
            "\u001b[34m\u001b[1mval: \u001b[0mFast image access ✅ (ping: 0.0±0.0 ms, read: 390.0±186.6 MB/s, size: 13.1 KB)\n",
            "\u001b[K\u001b[34m\u001b[1mval: \u001b[0mScanning /content/isl_dataset/data/val... 2522 images, 0 corrupt: 100% ━━━━━━━━━━━━ 2522/2522 2.2Kit/s 1.1s\n",
            "\u001b[34m\u001b[1mval: \u001b[0mNew cache created: /content/isl_dataset/data/val.cache\n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
            "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000333, momentum=0.9) with parameter groups 82 weight(decay=0.0), 83 weight(decay=0.0005), 83 bias(decay=0.0)\n",
            "Image sizes 224 train, 224 val\n",
            "Using 2 dataloader workers\n",
            "Logging results to \u001b[1m/content/runs/classify/train\u001b[0m\n",
            "Starting training for 5 epochs...\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[KDownloading https://ultralytics.com/assets/Arial.ttf to '/root/.config/Ultralytics/Arial.ttf': 100% ━━━━━━━━━━━━ 755.1KB 21.6MB/s 0.0s\n",
            "\u001b[K        1/5      1.97G       2.39          9        224: 100% ━━━━━━━━━━━━ 475/475 6.5it/s 1:14\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 79/79 12.6it/s 6.3s\n",
            "                   all      0.888      0.985\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K        2/5      2.29G     0.4563          9        224: 100% ━━━━━━━━━━━━ 475/475 6.7it/s 1:11\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 79/79 13.0it/s 6.1s\n",
            "                   all      0.966      0.999\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K        3/5      2.35G     0.2485          9        224: 100% ━━━━━━━━━━━━ 475/475 7.6it/s 1:03\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 79/79 14.6it/s 5.4s\n",
            "                   all      0.967          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K        4/5      2.41G      0.172          9        224: 100% ━━━━━━━━━━━━ 475/475 7.7it/s 1:02\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 79/79 15.3it/s 5.2s\n",
            "                   all      0.989          1\n",
            "\n",
            "      Epoch    GPU_mem       loss  Instances       Size\n",
            "\u001b[K        5/5      2.46G     0.1154          9        224: 100% ━━━━━━━━━━━━ 475/475 7.7it/s 1:02\n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 79/79 15.1it/s 5.2s\n",
            "                   all      0.991          1\n",
            "\n",
            "5 epochs completed in 0.106 hours.\n",
            "Optimizer stripped from /content/runs/classify/train/weights/last.pt, 57.1MB\n",
            "Optimizer stripped from /content/runs/classify/train/weights/best.pt, 57.1MB\n",
            "\n",
            "Validating /content/runs/classify/train/weights/best.pt...\n",
            "Ultralytics 8.3.202 🚀 Python-3.12.11 torch-2.8.0+cu126 CUDA:0 (Tesla T4, 15095MiB)\n",
            "YOLO11x-cls summary (fused): 94 layers, 28,365,722 parameters, 0 gradients, 110.4 GFLOPs\n",
            "\u001b[34m\u001b[1mtrain:\u001b[0m /content/isl_dataset/data/train... found 7593 images in 26 classes ✅ \n",
            "\u001b[34m\u001b[1mval:\u001b[0m /content/isl_dataset/data/val... found 2522 images in 26 classes ✅ \n",
            "\u001b[34m\u001b[1mtest:\u001b[0m /content/isl_dataset/data/test... found 2522 images in 26 classes ✅ \n",
            "\u001b[K               classes   top1_acc   top5_acc: 100% ━━━━━━━━━━━━ 79/79 13.2it/s 6.0s\n",
            "                   all      0.991          1\n",
            "Speed: 0.1ms preprocess, 1.6ms inference, 0.0ms loss, 0.0ms postprocess per image\n",
            "Results saved to \u001b[1m/content/runs/classify/train\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Torch transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "train_ds = datasets.ImageFolder(train_dir, transform=transform)\n",
        "val_ds = datasets.ImageFolder(val_dir, transform=transform)\n",
        "test_ds = datasets.ImageFolder(test_dir, transform=transform)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
        "test_loader = DataLoader(test_ds, batch_size=32, shuffle=False)\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vit_model = ViTForImageClassification.from_pretrained(\n",
        "    'google/vit-base-patch16-224',\n",
        "    num_labels=len(train_ds.classes),\n",
        "    ignore_mismatched_sizes=True\n",
        ")\n",
        "vit_model.to(device)\n",
        "optimizer = Adam(vit_model.parameters(), lr=1e-4)\n",
        "criterion = nn.CrossEntropyLoss()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 275,
          "referenced_widgets": [
            "ab12f75ea19b4b3fbc1224f454e8f06b",
            "994ef9f8e8b849b8885f7aaebbc5c7e8",
            "b5ba55b73b524314afca895b6e4cea62",
            "1bf5aeb5dc0d497a99c3400bfa9a3492",
            "900282a1a4024be0b30dbfebebf93ac0",
            "4facaf9c9ed0467693c681b54c1af4f3",
            "1c20bee95c3b4c62b2b154da9e8a7f4f",
            "6d73859a44494e0fa5243126ecfdbe66",
            "53cd5a3fb99541369673c388cc899bf7",
            "e5b66069386a4684be3cbb16cb97e94e",
            "fee00fabb3e34d3c99ef179a17baff56",
            "605dde1ebfb74d59b1492ec7674af834",
            "f34f0ff074a74082b4a7034bf44c29c9",
            "5d84512c67064fcd93cc8d1b97cd217e",
            "70432b0e6992470d9d42412157fcac21",
            "8e0544fc4f1a43b5b3f32f066528ed3e",
            "0b60f9453f544d82b22065ce9853f8c9",
            "9b5c1001e95e4e14b565953f6a04b55e",
            "d61762833ca34f46b158a766a91718df",
            "cb3684cfd43d4a95a82ca5bb39ba657d",
            "2503f05b99c9421fa1e40dcc4a169560",
            "3af27f8df1aa471fb570b344b306fc67"
          ]
        },
        "id": "_yzh4GrfJzXf",
        "outputId": "03be5c11-6526-42dd-80d1-a501cec60f50"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "config.json: 0.00B [00:00, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "ab12f75ea19b4b3fbc1224f454e8f06b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "605dde1ebfb74d59b1492ec7674af834"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of ViTForImageClassification were not initialized from the model checkpoint at google/vit-base-patch16-224 and are newly initialized because the shapes did not match:\n",
            "- classifier.bias: found shape torch.Size([1000]) in the checkpoint and torch.Size([26]) in the model instantiated\n",
            "- classifier.weight: found shape torch.Size([1000, 768]) in the checkpoint and torch.Size([26, 768]) in the model instantiated\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 5\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    vit_model.train()\n",
        "    train_loss, correct = 0, 0\n",
        "\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = vit_model(images)\n",
        "        loss = criterion(outputs.logits, labels)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        train_loss += loss.item()\n",
        "        _, preds = torch.max(outputs.logits, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "\n",
        "    train_acc = 100 * correct / len(train_ds)\n",
        "    print(f\"Epoch {epoch+1}/{epochs} | Loss: {train_loss:.4f} | Accuracy: {train_acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e3lInUcp5Fz5",
        "outputId": "20b201cd-6b62-40ec-b780-f4f5e9f04c25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5 | Loss: 144.6501 | Accuracy: 86.42%\n",
            "Epoch 2/5 | Loss: 5.2283 | Accuracy: 99.54%\n",
            "Epoch 3/5 | Loss: 5.3551 | Accuracy: 99.50%\n",
            "Epoch 4/5 | Loss: 1.7183 | Accuracy: 99.83%\n",
            "Epoch 5/5 | Loss: 0.2741 | Accuracy: 100.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vit_model.eval()\n",
        "correct, total = 0, 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        outputs = vit_model(images)\n",
        "        _, preds = torch.max(outputs.logits, 1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "acc = 100 * correct / total\n",
        "print(f\"ViT Test Accuracy: {acc:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n4McVFmu_HD1",
        "outputId": "3a8b8dc3-816e-40d3-eab7-c73ebfa022c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ViT Test Accuracy: 99.72%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_ds_tf = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    train_dir, image_size=(224,224), batch_size=16, shuffle=True\n",
        ")\n",
        "val_ds_tf = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    val_dir, image_size=(224,224), batch_size=16, shuffle=True\n",
        ")\n",
        "test_ds_tf = tf.keras.preprocessing.image_dataset_from_directory(\n",
        "    test_dir, image_size=(224,224), batch_size=16, shuffle=False\n",
        ")\n",
        "\n",
        "num_classes = len(train_ds_tf.class_names)\n",
        "print(\"Classes:\", train_ds_tf.class_names)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RfiuxunYEDUK",
        "outputId": "f54b0595-d59f-4dbe-d8e4-db05a306a8d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found 7593 files belonging to 26 classes.\n",
            "Found 2522 files belonging to 26 classes.\n",
            "Found 2522 files belonging to 26 classes.\n",
            "Classes: ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_model(c16, c32, c64, d128, d64, d32, num_classes):\n",
        "    if c16==0 and c32==0 and c64==0: c64=1\n",
        "    if d128==0 and d64==0 and d32==0: d128=1\n",
        "\n",
        "    model = Sequential([layers.Input(shape=[224,224,3]), layers.Rescaling(1./255)])\n",
        "\n",
        "    for _ in range(c16):\n",
        "        model.add(layers.Conv2D(16,3,padding='same',activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "    for _ in range(c32):\n",
        "        model.add(layers.Conv2D(32,3,padding='same',activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "    for _ in range(c64):\n",
        "        model.add(layers.Conv2D(64,3,padding='same',activation='relu'))\n",
        "        model.add(layers.MaxPooling2D())\n",
        "\n",
        "    model.add(layers.Flatten())\n",
        "\n",
        "    for _ in range(d128): model.add(layers.Dense(128,activation='relu'))\n",
        "    for _ in range(d64): model.add(layers.Dense(64,activation='relu'))\n",
        "    for _ in range(d32): model.add(layers.Dense(32,activation='relu'))\n",
        "\n",
        "    model.add(layers.Dense(num_classes))\n",
        "    return model\n",
        "\n",
        "model_cnn = get_model(1,1,1,1,0,0,num_classes)\n",
        "model_cnn.compile(optimizer='adam', loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True), metrics=['accuracy'])\n",
        "history = model_cnn.fit(train_ds_tf, validation_data=val_ds_tf, epochs=5)\n",
        "loss, acc = model_cnn.evaluate(test_ds_tf)\n",
        "print(f\"CNN Test Accuracy: {acc*100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3s4iInKEKgx",
        "outputId": "c1a250ad-0e43-4c90-f168-2318837f5c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m18s\u001b[0m 26ms/step - accuracy: 0.2529 - loss: 2.6138 - val_accuracy: 0.8204 - val_loss: 0.6050\n",
            "Epoch 2/5\n",
            "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 19ms/step - accuracy: 0.8714 - loss: 0.4246 - val_accuracy: 0.9175 - val_loss: 0.2964\n",
            "Epoch 3/5\n",
            "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9713 - loss: 0.0967 - val_accuracy: 0.9266 - val_loss: 0.2902\n",
            "Epoch 4/5\n",
            "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 17ms/step - accuracy: 0.9788 - loss: 0.0739 - val_accuracy: 0.9274 - val_loss: 0.3242\n",
            "Epoch 5/5\n",
            "\u001b[1m475/475\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 18ms/step - accuracy: 0.9828 - loss: 0.0509 - val_accuracy: 0.9266 - val_loss: 0.3328\n",
            "\u001b[1m158/158\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 8ms/step - accuracy: 0.9257 - loss: 0.3409\n",
            "CNN Test Accuracy: 91.83%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 604
        },
        "id": "Nv8VoyStEQfk",
        "outputId": "320763be-5f0c-4eaa-8125-5916342db840"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using labels from train_ds_tf.class_names (TF).\n",
            "Using labels from train_ds (torch ImageFolder).\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const streamerId = 'colab-isl-streamer';\n",
              "if (!document.getElementById(streamerId)) {\n",
              "  const container = document.createElement('div');\n",
              "  container.id = streamerId;\n",
              "  container.style = 'margin:10px 0;';\n",
              "\n",
              "  const startBtn = document.createElement('button');\n",
              "  startBtn.textContent = 'Start Webcam (Predict)';\n",
              "  startBtn.style = 'margin-right:10px;';\n",
              "  const stopBtn = document.createElement('button');\n",
              "  stopBtn.textContent = 'Stop Webcam';\n",
              "  stopBtn.disabled = true;\n",
              "  stopBtn.style = 'margin-right:10px;';\n",
              "\n",
              "  const statusSpan = document.createElement('span');\n",
              "  statusSpan.textContent = 'Status: idle';\n",
              "  statusSpan.style = 'margin-left:10px; font-weight:600;';\n",
              "\n",
              "  const video = document.createElement('video');\n",
              "  video.autoplay = true;\n",
              "  video.playsInline = true;\n",
              "  video.style = 'display:block; margin-top:10px; max-width:480px; border:1px solid #ccc;';\n",
              "\n",
              "  container.appendChild(startBtn);\n",
              "  container.appendChild(stopBtn);\n",
              "  container.appendChild(statusSpan);\n",
              "  container.appendChild(video);\n",
              "  document.body.appendChild(container);\n",
              "\n",
              "  let stream = null;\n",
              "  let intervalId = null;\n",
              "\n",
              "  startBtn.onclick = async () => {\n",
              "    try {\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: true, audio: false});\n",
              "      video.srcObject = stream;\n",
              "      statusSpan.textContent = 'Status: streaming';\n",
              "      startBtn.disabled = true;\n",
              "      stopBtn.disabled = false;\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      const ctx = canvas.getContext('2d');\n",
              "\n",
              "      intervalId = setInterval(() => {\n",
              "        try {\n",
              "          canvas.width = video.videoWidth || 640;\n",
              "          canvas.height = video.videoHeight || 480;\n",
              "          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
              "          const data = canvas.toDataURL('image/jpeg', 0.6);\n",
              "          google.colab.kernel.invokeFunction('notebook.receiveFrameForPrediction', [data], {});\n",
              "        } catch(err) {\n",
              "          console.log('frame send error', err);\n",
              "        }\n",
              "      }, 700); // one frame every 700ms\n",
              "    } catch (err) {\n",
              "      statusSpan.textContent = 'Status: permission denied / error';\n",
              "      console.error(err);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  stopBtn.onclick = () => {\n",
              "    if (intervalId) {\n",
              "      clearInterval(intervalId);\n",
              "      intervalId = null;\n",
              "    }\n",
              "    if (stream) {\n",
              "      stream.getTracks().forEach(t => t.stop());\n",
              "      stream = null;\n",
              "    }\n",
              "    video.srcObject = null;\n",
              "    statusSpan.textContent = 'Status: stopped';\n",
              "    startBtn.disabled = false;\n",
              "    stopBtn.disabled = true;\n",
              "  };\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webcam UI injected. Use the 'Start Webcam (Predict)' button that appears above this cell to begin streaming.\n",
            "Predictions (TF and ViT) will be printed below as frames are processed.\n",
            "[2025-09-21 16:41:17] Final -> t (score=0.590) | TF: t(0.590) | ViT: l(0.241)\n",
            "[2025-09-21 16:41:18] Final -> q (score=0.850) | TF: t(0.592) | ViT: q(0.850)\n",
            "[2025-09-21 16:41:19] Final -> q (score=0.940) | TF: t(0.589) | ViT: q(0.940)\n",
            "[2025-09-21 16:41:20] Final -> t (score=0.590) | TF: t(0.590) | ViT: k(0.450)\n",
            "[2025-09-21 16:41:20] Final -> t (score=0.584) | TF: t(0.584) | ViT: q(0.569)\n",
            "[2025-09-21 16:41:21] Final -> q (score=0.864) | TF: t(0.585) | ViT: q(0.864)\n",
            "[2025-09-21 16:41:22] Final -> q (score=0.938) | TF: t(0.584) | ViT: q(0.938)\n",
            "[2025-09-21 16:41:23] Final -> t (score=0.589) | TF: t(0.589) | ViT: l(0.198)\n",
            "[2025-09-21 16:41:23] Final -> t (score=0.589) | TF: t(0.589) | ViT: l(0.155)\n",
            "[2025-09-21 16:41:23] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.159)\n",
            "[2025-09-21 16:41:24] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.177)\n",
            "[2025-09-21 16:41:25] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.171)\n",
            "[2025-09-21 16:41:26] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.169)\n",
            "[2025-09-21 16:41:26] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.145)\n",
            "[2025-09-21 16:41:27] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.155)\n",
            "[2025-09-21 16:41:27] Final -> t (score=0.589) | TF: t(0.589) | ViT: i(0.168)\n",
            "[2025-09-21 16:41:28] Final -> t (score=0.590) | TF: t(0.590) | ViT: i(0.180)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ------------------ Webcam live ISL overlay ------------------\n",
        "from google.colab import output\n",
        "from base64 import b64decode\n",
        "import io\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import IPython.display as display\n",
        "import tensorflow as tf\n",
        "\n",
        "# --- Transforms for ViT ---\n",
        "vit_transform = T.Compose([\n",
        "    T.Resize((224,224)),\n",
        "    T.ToTensor(),\n",
        "    T.Normalize(mean=[0.5,0.5,0.5], std=[0.5,0.5,0.5])\n",
        "])\n",
        "\n",
        "# --- Labels ---\n",
        "tf_labels = train_ds_tf.class_names  # TensorFlow CNN labels\n",
        "vit_labels = train_ds.classes  # ViT labels\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "vit_model.to(device)\n",
        "vit_model.eval()  # Make sure model is in eval mode\n",
        "\n",
        "# --- Prediction callback ---\n",
        "def _process_frame_overlay(b64_img):\n",
        "    try:\n",
        "        img_bytes = b64decode(b64_img.split(',')[1])\n",
        "        pil_img = PIL.Image.open(io.BytesIO(img_bytes)).convert('RGB')\n",
        "\n",
        "        # TensorFlow CNN\n",
        "        tf_input = np.expand_dims(np.array(pil_img.resize((224,224)))/255.0, axis=0)\n",
        "        tf_logits = model_cnn.predict(tf_input, verbose=0) # Corrected model name to model_cnn\n",
        "        tf_idx = int(np.argmax(tf_logits, axis=1)[0])\n",
        "        tf_label = tf_labels[tf_idx]\n",
        "        tf_conf = float(np.max(tf.nn.softmax(tf_logits).numpy()))\n",
        "\n",
        "        # ViT PyTorch\n",
        "        vit_img = vit_transform(pil_img).unsqueeze(0).to(device)\n",
        "        with torch.no_grad():\n",
        "            out = vit_model(vit_img)\n",
        "            probs = torch.softmax(out.logits, dim=1).cpu().numpy()[0]\n",
        "            vit_idx = int(np.argmax(probs))\n",
        "            vit_label = vit_labels[vit_idx]\n",
        "            vit_conf = float(probs[vit_idx])\n",
        "\n",
        "        # Choose final label (higher confidence)\n",
        "        final_label = tf_label if tf_conf >= vit_conf else vit_label\n",
        "        return str(final_label)\n",
        "    except Exception as e: # Added exception handling for debugging\n",
        "        print(\"Prediction overlay error:\", e)\n",
        "        return \"Error\"\n",
        "\n",
        "output.register_callback('notebook.overlayPredict', _process_frame_overlay)\n",
        "\n",
        "# ---------------------- Inject JS for webcam streaming with overlay ----------------------\n",
        "js_overlay = \"\"\"\n",
        "const streamerOverlayId = 'colab-isl-streamer-overlay';\n",
        "if (!document.getElementById(streamerOverlayId)) {\n",
        "  const container = document.createElement('div');\n",
        "  container.id = streamerOverlayId;\n",
        "  container.style = 'margin:10px 0;';\n",
        "\n",
        "  const startBtn = document.createElement('button');\n",
        "  startBtn.textContent = 'Start Webcam (Overlay Predict)';\n",
        "  startBtn.style = 'margin-right:10px;';\n",
        "  const stopBtn = document.createElement('button');\n",
        "  stopBtn.textContent = 'Stop Webcam Overlay';\n",
        "  stopBtn.disabled = true;\n",
        "  stopBtn.style = 'margin-right:10px;';\n",
        "\n",
        "  const statusSpan = document.createElement('span');\n",
        "  statusSpan.textContent = 'Status: idle';\n",
        "  statusSpan.style = 'margin-left:10px; font-weight:600;';\n",
        "\n",
        "  const video = document.createElement('video');\n",
        "  video.autoplay = true;\n",
        "  video.playsInline = true;\n",
        "  video.style = 'display:block; margin-top:10px; max-width:480px; border:1px solid #ccc;';\n",
        "\n",
        "  const overlay = document.createElement('div');\n",
        "  overlay.style = `\n",
        "    position: absolute;\n",
        "    top: 0;\n",
        "    left: 0;\n",
        "    background-color: rgba(255, 255, 255, 0.7);\n",
        "    color: black;\n",
        "    font-size: 24px;\n",
        "    padding: 5px;\n",
        "    z-index: 10;\n",
        "  `;\n",
        "  const overlayText = document.createElement('span');\n",
        "  overlay.appendChild(overlayText);\n",
        "\n",
        "  const videoContainer = document.createElement('div');\n",
        "  videoContainer.style = 'position: relative; display: inline-block;';\n",
        "  videoContainer.appendChild(video);\n",
        "  videoContainer.appendChild(overlay);\n",
        "\n",
        "\n",
        "  container.appendChild(startBtn);\n",
        "  container.appendChild(stopBtn);\n",
        "  container.appendChild(statusSpan);\n",
        "  container.appendChild(videoContainer);\n",
        "  document.body.appendChild(container);\n",
        "\n",
        "\n",
        "  let stream = null;\n",
        "  let intervalId = null;\n",
        "\n",
        "  startBtn.onclick = async () => {\n",
        "    try {\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video: true, audio: false});\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "      // Adjust overlay size and position after video starts\n",
        "      overlay.style.width = video.offsetWidth + 'px';\n",
        "      overlay.style.height = 'auto'; // or set a fixed height\n",
        "      overlay.style.top = (video.offsetTop + video.offsetHeight - overlay.offsetHeight) + 'px';\n",
        "      overlay.style.left = video.offsetLeft + 'px';\n",
        "      overlayText.textContent = 'Loading...'; // Initial text\n",
        "\n",
        "      statusSpan.textContent = 'Status: streaming';\n",
        "      startBtn.disabled = true;\n",
        "      stopBtn.disabled = false;\n",
        "\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      const ctx = canvas.getContext('2d');\n",
        "\n",
        "      intervalId = setInterval(() => {\n",
        "        try {\n",
        "          if (!video.videoWidth || !video.videoHeight) return; // Avoid processing before video is ready\n",
        "\n",
        "          canvas.width = video.videoWidth;\n",
        "          canvas.height = video.videoHeight;\n",
        "          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
        "          const data = canvas.toDataURL('image/jpeg', 0.6);\n",
        "\n",
        "          google.colab.kernel.invokeFunction('notebook.overlayPredict', [data], {})\n",
        "            .then(result => {\n",
        "              overlayText.textContent = result.data[0]; // Update overlay with prediction\n",
        "               // Adjust overlay vertical position again if necessary\n",
        "              overlay.style.top = (video.offsetTop + video.offsetHeight - overlay.offsetHeight) + 'px';\n",
        "            });\n",
        "\n",
        "        } catch(err) {\n",
        "          console.log('overlay frame send error', err);\n",
        "        }\n",
        "      }, 700); // one frame every 700ms\n",
        "    } catch (err) {\n",
        "      statusSpan.textContent = 'Status: permission denied / error';\n",
        "      console.error(err);\n",
        "    }\n",
        "  };\n",
        "\n",
        "  stopBtn.onclick = () => {\n",
        "    if (intervalId) {\n",
        "      clearInterval(intervalId);\n",
        "      intervalId = null;\n",
        "    }\n",
        "    if (stream) {\n",
        "      stream.getTracks().forEach(t => t.stop());\n",
        "      stream = null;\n",
        "    }\n",
        "    video.srcObject = null;\n",
        "    statusSpan.textContent = 'Status: stopped';\n",
        "    startBtn.disabled = false;\n",
        "    stopBtn.disabled = true;\n",
        "    overlayText.textContent = ''; // Clear overlay text on stop\n",
        "  };\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "display.display(display.Javascript(js_overlay))\n",
        "print(\"Webcam Overlay UI injected. Use the 'Start Webcam (Overlay Predict)' button that appears above this cell.\")\n",
        "print(\"Predictions (TF and ViT combined) will be overlaid on the video stream.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "VC9eAibkFA6c",
        "outputId": "2e694ec0-ba8c-4db7-f04d-c5928421e763"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "const streamerOverlayId = 'colab-isl-streamer-overlay';\n",
              "if (!document.getElementById(streamerOverlayId)) {\n",
              "  const container = document.createElement('div');\n",
              "  container.id = streamerOverlayId;\n",
              "  container.style = 'margin:10px 0;';\n",
              "\n",
              "  const startBtn = document.createElement('button');\n",
              "  startBtn.textContent = 'Start Webcam (Overlay Predict)';\n",
              "  startBtn.style = 'margin-right:10px;';\n",
              "  const stopBtn = document.createElement('button');\n",
              "  stopBtn.textContent = 'Stop Webcam Overlay';\n",
              "  stopBtn.disabled = true;\n",
              "  stopBtn.style = 'margin-right:10px;';\n",
              "\n",
              "  const statusSpan = document.createElement('span');\n",
              "  statusSpan.textContent = 'Status: idle';\n",
              "  statusSpan.style = 'margin-left:10px; font-weight:600;';\n",
              "\n",
              "  const video = document.createElement('video');\n",
              "  video.autoplay = true;\n",
              "  video.playsInline = true;\n",
              "  video.style = 'display:block; margin-top:10px; max-width:480px; border:1px solid #ccc;';\n",
              "\n",
              "  const overlay = document.createElement('div');\n",
              "  overlay.style = `\n",
              "    position: absolute;\n",
              "    top: 0;\n",
              "    left: 0;\n",
              "    background-color: rgba(255, 255, 255, 0.7);\n",
              "    color: black;\n",
              "    font-size: 24px;\n",
              "    padding: 5px;\n",
              "    z-index: 10;\n",
              "  `;\n",
              "  const overlayText = document.createElement('span');\n",
              "  overlay.appendChild(overlayText);\n",
              "\n",
              "  const videoContainer = document.createElement('div');\n",
              "  videoContainer.style = 'position: relative; display: inline-block;';\n",
              "  videoContainer.appendChild(video);\n",
              "  videoContainer.appendChild(overlay);\n",
              "\n",
              "\n",
              "  container.appendChild(startBtn);\n",
              "  container.appendChild(stopBtn);\n",
              "  container.appendChild(statusSpan);\n",
              "  container.appendChild(videoContainer);\n",
              "  document.body.appendChild(container);\n",
              "\n",
              "\n",
              "  let stream = null;\n",
              "  let intervalId = null;\n",
              "\n",
              "  startBtn.onclick = async () => {\n",
              "    try {\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video: true, audio: false});\n",
              "      video.srcObject = stream;\n",
              "      await video.play();\n",
              "      // Adjust overlay size and position after video starts\n",
              "      overlay.style.width = video.offsetWidth + 'px';\n",
              "      overlay.style.height = 'auto'; // or set a fixed height\n",
              "      overlay.style.top = (video.offsetTop + video.offsetHeight - overlay.offsetHeight) + 'px';\n",
              "      overlay.style.left = video.offsetLeft + 'px';\n",
              "      overlayText.textContent = 'Loading...'; // Initial text\n",
              "\n",
              "      statusSpan.textContent = 'Status: streaming';\n",
              "      startBtn.disabled = true;\n",
              "      stopBtn.disabled = false;\n",
              "\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      const ctx = canvas.getContext('2d');\n",
              "\n",
              "      intervalId = setInterval(() => {\n",
              "        try {\n",
              "          if (!video.videoWidth || !video.videoHeight) return; // Avoid processing before video is ready\n",
              "\n",
              "          canvas.width = video.videoWidth;\n",
              "          canvas.height = video.videoHeight;\n",
              "          ctx.drawImage(video, 0, 0, canvas.width, canvas.height);\n",
              "          const data = canvas.toDataURL('image/jpeg', 0.6);\n",
              "\n",
              "          google.colab.kernel.invokeFunction('notebook.overlayPredict', [data], {})\n",
              "            .then(result => {\n",
              "              overlayText.textContent = result.data[0]; // Update overlay with prediction\n",
              "               // Adjust overlay vertical position again if necessary\n",
              "              overlay.style.top = (video.offsetTop + video.offsetHeight - overlay.offsetHeight) + 'px';\n",
              "            });\n",
              "\n",
              "        } catch(err) {\n",
              "          console.log('overlay frame send error', err);\n",
              "        }\n",
              "      }, 700); // one frame every 700ms\n",
              "    } catch (err) {\n",
              "      statusSpan.textContent = 'Status: permission denied / error';\n",
              "      console.error(err);\n",
              "    }\n",
              "  };\n",
              "\n",
              "  stopBtn.onclick = () => {\n",
              "    if (intervalId) {\n",
              "      clearInterval(intervalId);\n",
              "      intervalId = null;\n",
              "    }\n",
              "    if (stream) {\n",
              "      stream.getTracks().forEach(t => t.stop());\n",
              "      stream = null;\n",
              "    }\n",
              "    video.srcObject = null;\n",
              "    statusSpan.textContent = 'Status: stopped';\n",
              "    startBtn.disabled = false;\n",
              "    stopBtn.disabled = true;\n",
              "    overlayText.textContent = ''; // Clear overlay text on stop\n",
              "  };\n",
              "}\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webcam Overlay UI injected. Use the 'Start Webcam (Overlay Predict)' button that appears above this cell.\n",
            "Predictions (TF and ViT combined) will be overlaid on the video stream.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "js_code = r\"\"\"\n",
        "(async function() {\n",
        "  const containerId = 'isl-overlay-streamer';\n",
        "  if (document.getElementById(containerId)) return;\n",
        "  const container = document.createElement('div');\n",
        "  container.id = containerId;\n",
        "  container.style = 'margin:10px 0; position: relative;';\n",
        "\n",
        "  const startBtn = document.createElement('button');\n",
        "  startBtn.textContent = 'Start Webcam (Overlay)';\n",
        "  startBtn.style = 'margin-right:10px;';\n",
        "  const stopBtn = document.createElement('button');\n",
        "  stopBtn.textContent = 'Stop Webcam';\n",
        "  stopBtn.disabled = true;\n",
        "  stopBtn.style = 'margin-right:10px;';\n",
        "\n",
        "  const statusSpan = document.createElement('span');\n",
        "  statusSpan.textContent = 'Status: idle';\n",
        "  statusSpan.style = 'margin-left:10px; font-weight:600;';\n",
        "\n",
        "  const video = document.createElement('video');\n",
        "  video.autoplay = true;\n",
        "  video.playsInline = true;\n",
        "  video.style = 'display:block; margin-top:10px; max-width:480px; border:1px solid #ccc;';\n",
        "\n",
        "  const overlay = document.createElement('div');\n",
        "  overlay.style.position = 'absolute';\n",
        "  overlay.style.top = '20px';\n",
        "  overlay.style.left = '20px';\n",
        "  overlay.style.color = 'red';\n",
        "  overlay.style.fontSize = '60px';\n",
        "  overlay.style.fontWeight = 'bold';\n",
        "  overlay.style.textShadow = '2px 2px black';\n",
        "  overlay.innerText = '';\n",
        "\n",
        "  container.appendChild(startBtn);\n",
        "  container.appendChild(stopBtn);\n",
        "  container.appendChild(statusSpan);\n",
        "  container.appendChild(video);\n",
        "  container.appendChild(overlay);\n",
        "  document.body.appendChild(container);\n",
        "\n",
        "  let stream=null, intervalId=null;\n",
        "\n",
        "  startBtn.onclick = async () => {\n",
        "    try {\n",
        "      stream = await navigator.mediaDevices.getUserMedia({video:true});\n",
        "      video.srcObject = stream;\n",
        "      statusSpan.textContent='Status: streaming';\n",
        "      startBtn.disabled=true;\n",
        "      stopBtn.disabled=false;\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      const ctx = canvas.getContext('2d');\n",
        "\n",
        "      intervalId = setInterval(async () => {\n",
        "        canvas.width = video.videoWidth || 640;\n",
        "        canvas.height = video.videoHeight || 480;\n",
        "        ctx.drawImage(video,0,0,canvas.width,canvas.height);\n",
        "        const data = canvas.toDataURL('image/jpeg',0.6);\n",
        "\n",
        "        // Call Python callback and get prediction\n",
        "        const result = await google.colab.kernel.invokeFunction('notebook.overlayPredict',[data],{});\n",
        "        overlay.innerText = result.value;  // Display predicted letter\n",
        "      }, 700);\n",
        "    } catch(err){ statusSpan.textContent='Status: error'; console.error(err);}\n",
        "  };\n",
        "\n",
        "  stopBtn.onclick = () => {\n",
        "    if(intervalId){ clearInterval(intervalId); intervalId=null; }\n",
        "    if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }\n",
        "    video.srcObject=null;\n",
        "    overlay.innerText='';\n",
        "    statusSpan.textContent='Status: stopped';\n",
        "    startBtn.disabled=false; stopBtn.disabled=true;\n",
        "  };\n",
        "})();\n",
        "\"\"\"\n",
        "display.display(display.Javascript(js_code))\n",
        "print(\"Webcam overlay UI injected. Click 'Start Webcam (Overlay)' to begin live ISL recognition.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "ZSO5oE2BHhok",
        "outputId": "f61c74dc-dffc-4da5-bad8-8dbc35aff8a6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "(async function() {\n",
              "  const containerId = 'isl-overlay-streamer';\n",
              "  if (document.getElementById(containerId)) return;\n",
              "  const container = document.createElement('div');\n",
              "  container.id = containerId;\n",
              "  container.style = 'margin:10px 0; position: relative;';\n",
              "\n",
              "  const startBtn = document.createElement('button');\n",
              "  startBtn.textContent = 'Start Webcam (Overlay)';\n",
              "  startBtn.style = 'margin-right:10px;';\n",
              "  const stopBtn = document.createElement('button');\n",
              "  stopBtn.textContent = 'Stop Webcam';\n",
              "  stopBtn.disabled = true;\n",
              "  stopBtn.style = 'margin-right:10px;';\n",
              "\n",
              "  const statusSpan = document.createElement('span');\n",
              "  statusSpan.textContent = 'Status: idle';\n",
              "  statusSpan.style = 'margin-left:10px; font-weight:600;';\n",
              "\n",
              "  const video = document.createElement('video');\n",
              "  video.autoplay = true;\n",
              "  video.playsInline = true;\n",
              "  video.style = 'display:block; margin-top:10px; max-width:480px; border:1px solid #ccc;';\n",
              "\n",
              "  const overlay = document.createElement('div');\n",
              "  overlay.style.position = 'absolute';\n",
              "  overlay.style.top = '20px';\n",
              "  overlay.style.left = '20px';\n",
              "  overlay.style.color = 'red';\n",
              "  overlay.style.fontSize = '60px';\n",
              "  overlay.style.fontWeight = 'bold';\n",
              "  overlay.style.textShadow = '2px 2px black';\n",
              "  overlay.innerText = '';\n",
              "\n",
              "  container.appendChild(startBtn);\n",
              "  container.appendChild(stopBtn);\n",
              "  container.appendChild(statusSpan);\n",
              "  container.appendChild(video);\n",
              "  container.appendChild(overlay);\n",
              "  document.body.appendChild(container);\n",
              "\n",
              "  let stream=null, intervalId=null;\n",
              "\n",
              "  startBtn.onclick = async () => {\n",
              "    try {\n",
              "      stream = await navigator.mediaDevices.getUserMedia({video:true});\n",
              "      video.srcObject = stream;\n",
              "      statusSpan.textContent='Status: streaming';\n",
              "      startBtn.disabled=true;\n",
              "      stopBtn.disabled=false;\n",
              "\n",
              "      const canvas = document.createElement('canvas');\n",
              "      const ctx = canvas.getContext('2d');\n",
              "\n",
              "      intervalId = setInterval(async () => {\n",
              "        canvas.width = video.videoWidth || 640;\n",
              "        canvas.height = video.videoHeight || 480;\n",
              "        ctx.drawImage(video,0,0,canvas.width,canvas.height);\n",
              "        const data = canvas.toDataURL('image/jpeg',0.6);\n",
              "\n",
              "        // Call Python callback and get prediction\n",
              "        const result = await google.colab.kernel.invokeFunction('notebook.overlayPredict',[data],{});\n",
              "        overlay.innerText = result.value;  // Display predicted letter\n",
              "      }, 700);\n",
              "    } catch(err){ statusSpan.textContent='Status: error'; console.error(err);}\n",
              "  };\n",
              "\n",
              "  stopBtn.onclick = () => {\n",
              "    if(intervalId){ clearInterval(intervalId); intervalId=null; }\n",
              "    if(stream){ stream.getTracks().forEach(t=>t.stop()); stream=null; }\n",
              "    video.srcObject=null;\n",
              "    overlay.innerText='';\n",
              "    statusSpan.textContent='Status: stopped';\n",
              "    startBtn.disabled=false; stopBtn.disabled=true;\n",
              "  };\n",
              "})();\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Webcam overlay UI injected. Click 'Start Webcam (Overlay)' to begin live ISL recognition.\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "ab12f75ea19b4b3fbc1224f454e8f06b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_994ef9f8e8b849b8885f7aaebbc5c7e8",
              "IPY_MODEL_b5ba55b73b524314afca895b6e4cea62",
              "IPY_MODEL_1bf5aeb5dc0d497a99c3400bfa9a3492"
            ],
            "layout": "IPY_MODEL_900282a1a4024be0b30dbfebebf93ac0"
          }
        },
        "994ef9f8e8b849b8885f7aaebbc5c7e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4facaf9c9ed0467693c681b54c1af4f3",
            "placeholder": "​",
            "style": "IPY_MODEL_1c20bee95c3b4c62b2b154da9e8a7f4f",
            "value": "config.json: "
          }
        },
        "b5ba55b73b524314afca895b6e4cea62": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6d73859a44494e0fa5243126ecfdbe66",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_53cd5a3fb99541369673c388cc899bf7",
            "value": 1
          }
        },
        "1bf5aeb5dc0d497a99c3400bfa9a3492": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e5b66069386a4684be3cbb16cb97e94e",
            "placeholder": "​",
            "style": "IPY_MODEL_fee00fabb3e34d3c99ef179a17baff56",
            "value": " 69.7k/? [00:00&lt;00:00, 5.82MB/s]"
          }
        },
        "900282a1a4024be0b30dbfebebf93ac0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4facaf9c9ed0467693c681b54c1af4f3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1c20bee95c3b4c62b2b154da9e8a7f4f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6d73859a44494e0fa5243126ecfdbe66": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": "20px"
          }
        },
        "53cd5a3fb99541369673c388cc899bf7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e5b66069386a4684be3cbb16cb97e94e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fee00fabb3e34d3c99ef179a17baff56": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "605dde1ebfb74d59b1492ec7674af834": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f34f0ff074a74082b4a7034bf44c29c9",
              "IPY_MODEL_5d84512c67064fcd93cc8d1b97cd217e",
              "IPY_MODEL_70432b0e6992470d9d42412157fcac21"
            ],
            "layout": "IPY_MODEL_8e0544fc4f1a43b5b3f32f066528ed3e"
          }
        },
        "f34f0ff074a74082b4a7034bf44c29c9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_0b60f9453f544d82b22065ce9853f8c9",
            "placeholder": "​",
            "style": "IPY_MODEL_9b5c1001e95e4e14b565953f6a04b55e",
            "value": "model.safetensors: 100%"
          }
        },
        "5d84512c67064fcd93cc8d1b97cd217e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d61762833ca34f46b158a766a91718df",
            "max": 346293852,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cb3684cfd43d4a95a82ca5bb39ba657d",
            "value": 346293852
          }
        },
        "70432b0e6992470d9d42412157fcac21": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2503f05b99c9421fa1e40dcc4a169560",
            "placeholder": "​",
            "style": "IPY_MODEL_3af27f8df1aa471fb570b344b306fc67",
            "value": " 346M/346M [00:06&lt;00:00, 51.4MB/s]"
          }
        },
        "8e0544fc4f1a43b5b3f32f066528ed3e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0b60f9453f544d82b22065ce9853f8c9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9b5c1001e95e4e14b565953f6a04b55e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d61762833ca34f46b158a766a91718df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cb3684cfd43d4a95a82ca5bb39ba657d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2503f05b99c9421fa1e40dcc4a169560": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3af27f8df1aa471fb570b344b306fc67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}