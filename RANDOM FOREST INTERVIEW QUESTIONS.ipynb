{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6f1ff338-c970-40ff-930c-92f360a75e2f",
   "metadata": {},
   "source": [
    "1. Explain Bagging and Boosting Methods. How Are They Different?\n",
    "==Bagging (Bootstrap Aggregating)\n",
    "Definition: Bagging is an ensemble technique that builds multiple independent models (usually of the same type) in parallel and averages their predictions (for regression) or votes (for classification).\n",
    "\n",
    "How it works:\n",
    "\n",
    "Multiple datasets are created using bootstrap sampling (random sampling with replacement).\n",
    "\n",
    "Each model (e.g., decision tree) is trained on a different bootstrapped subset.\n",
    "\n",
    "Final prediction is made by majority vote or average.\n",
    "\n",
    "Goal: Reduce variance and avoid overfitting.\n",
    "\n",
    " Examples:\n",
    "\n",
    "Random Forest is a classic example of Bagging.\n",
    "\n",
    "==Boosting\n",
    "Definition: Boosting is an ensemble technique that builds models sequentially, where each model tries to correct the errors made by the previous ones.\n",
    "\n",
    "How it works:\n",
    "\n",
    "Initially, all data points are equally weighted.\n",
    "\n",
    "After each model is trained, more weight is given to the misclassified examples.\n",
    "\n",
    "Final prediction is a weighted sum or vote of all weak learners.\n",
    "\n",
    "Goal: Reduce bias and improve model accuracy.\n",
    "\n",
    "Examples:\n",
    "\n",
    "AdaBoost, Gradient Boosting, XGBoost, LightGBM."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f57f7df5-1630-4cf2-a267-5acb17f54d3a",
   "metadata": {},
   "source": [
    "2. Explain How to Handle Imbalance in the Data\n",
    "==Data Imbalance Problem\n",
    "Occurs when one class (majority) dominates the others (minority), causing the model to bias towards the majority class.\n",
    "\n",
    "Example: In fraud detection, 99% non-fraud vs. 1% fraud.\n",
    "\n",
    "==Techniques to Handle Imbalanced Data\n",
    "1. Resampling Methods\n",
    "Oversampling the minority class:\n",
    "\n",
    "Techniques like SMOTE (Synthetic Minority Oversampling Technique).\n",
    "\n",
    "Creates synthetic examples for the minority class.\n",
    "\n",
    "Undersampling the majority class:\n",
    "\n",
    "Randomly removes samples from the majority class.\n",
    "\n",
    "Risk of losing important information.\n",
    "\n",
    "Combination of both (e.g., SMOTE + Tomek Links).\n",
    "\n",
    "2. Using Class Weights\n",
    "Assign higher weights to the minority class in the loss function.\n",
    "\n",
    "Many ML models (e.g., logistic regression, SVM, XGBoost) support class weights.\n",
    "\n",
    "Helps penalize misclassification of minority class more.\n",
    "\n",
    "3. Anomaly Detection Models\n",
    "In extreme imbalance (e.g., fraud, rare diseases), use models designed for anomaly detection.\n",
    "\n",
    "4. Ensemble Methods\n",
    "Use Balanced Random Forest, EasyEnsemble, or Boosting with sampling to improve minority class predictions.\n",
    "\n",
    "5. Evaluation Metrics\n",
    "Use proper metrics like:\n",
    "\n",
    "Precision, Recall, F1-Score\n",
    "\n",
    "ROC-AUC\n",
    "\n",
    "Confusion Matrix\n",
    "\n",
    "Avoid relying only on accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "706c1710-352d-4299-95da-cc863b42b997",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
